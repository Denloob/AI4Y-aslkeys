{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv # for reading csv files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import cv2\n",
        "from cvzone.HandTrackingModule import HandDetector\n",
        "from IPython.display import clear_output # for clearing the output in the notebook\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import pickle\n",
        "\n",
        "# Create a list for labeling\n",
        "labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'sp', 'nothing']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show all of the asl letters\n",
        "\n",
        "def cut_frame(img, frame_size):\n",
        "    # cut frame_size pixel from the top, bottom, left and right\n",
        "    return img[frame_size:-frame_size, frame_size:-frame_size]\n",
        "\n",
        "f, axarr = plt.subplots(5, 6, figsize=(15, 15))\n",
        "r, c = 0, 0\n",
        "\n",
        "for i in range(len(labels)):\n",
        "    if (i % 6 == 0 and i != 0):\n",
        "        r += 1\n",
        "        c = 0\n",
        "    img = cv2.imread('data/asl_alphabet_train/asl_alphabet_train/' + labels[i] + '/' + labels[i] + '1000.jpg')\n",
        "    axarr[r, c].imshow(cut_frame(img, 3))\n",
        "    axarr[r, c].set_title(labels[i], fontsize=16, color='#ffffff')\n",
        "    axarr[r, c].axis('off')\n",
        "    c += 1\n",
        "    \n",
        "axarr[r, c].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate a csv file for the asl letters\n",
        "\n",
        "detector = HandDetector(detectionCon=0.8, maxHands=1) # Create a hand detector\n",
        "\n",
        "# add labels to the csv file\n",
        "with open('traindots.csv', 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # Add 'label' as the first column and 70 more columns for the 70 other values\n",
        "    header_arr = ['label']\n",
        "    for i in range(70):\n",
        "        header_arr.append(i)\n",
        "\n",
        "    writer.writerow(header_arr) # Write the header\n",
        "\n",
        "# add the data to the csv file\n",
        "with open('traindots.csv', 'a', encoding='UTF8', newline='') as f: # Open the file to append\n",
        "    writer = csv.writer(f) # Create a writer\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        for j in range(1, 3001):\n",
        "            image = cv2.imread('data/asl_alphabet_train/asl_alphabet_train/'+labels[i]+'/'+labels[i]+str(j)+'.jpg') # Read the image from the path\n",
        "\n",
        "            hands = detector.findHands(image, draw=True) # Detect the hand\n",
        "\n",
        "            if (hands and hands[0] and hands[0][0]): # If the hand is detected\n",
        "                insertData = hands[0][0]['lmList'] # Get the landmarks\n",
        "                insertData = np.asarray(insertData, dtype=object).reshape(1, -1) # Convert the landmarks to a numpy array\n",
        "\n",
        "                insertData = np.append(insertData, hands[0][0]['bbox']) # Get the bounding box\n",
        "                insertData = np.asarray(insertData, dtype=object).reshape(1, -1) # Convert the bounding box to a numpy array\n",
        "\n",
        "                insertData = np.append(insertData, hands[0][0]['center']) # Get the center\n",
        "                insertData = np.asarray(insertData, dtype=object).reshape(1, -1) # Convert the center to a numpy array\n",
        "                \n",
        "                if (hands[0][0]['type'] == 'Right'): # If the hand is right\n",
        "                    insertData = np.append(insertData, 1) # Insert 1\n",
        "                elif (hands[0][0]['type'] == 'Left'): # If the hand is left\n",
        "                    insertData = np.append(insertData, 0) # Insert 0\n",
        "\n",
        "                insertData = np.insert(insertData, 0, i) # Insert the label\n",
        "                writer.writerow(insertData) # Append the data to the csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "traindots_df = pd.read_csv('traindots.csv') # Read the csv file for training\n",
        "testdots_df = pd.read_csv('testdots.csv') # Read the csv file for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "letter_image_count = traindots_df['label'].value_counts()\n",
        "\n",
        "sorted_by_index = [i for i in letter_image_count.sort_index()]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(labels[:-1], sorted_by_index)\n",
        "ax.set_xlabel(\"Letters\")\n",
        "ax.set_ylabel(\"No. of images\")\n",
        "ax.set_facecolor((253/255, 177/255, 87/255))\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "KNN = KNeighborsClassifier(n_neighbors=5) # Create a KNN classifier\n",
        "\n",
        "\n",
        "x_train = traindots_df.loc[:, traindots_df.columns != 'label'].values # Get the training data\n",
        "y_train = traindots_df['label'].values # Get the training labels\n",
        "\n",
        "KNN.fit(x_train, y_train) # Train the classifier\n",
        "\n",
        "with open(\"aslearn.ai\", \"wb\") as f: # Open the file to write\n",
        "    pickle.dump(KNN, f) # Write the classifier to the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test = traindots_df.loc[:, testdots_df.columns != 'label'].values # Get the testing data\n",
        "y_test = traindots_df['label'].values # Get the testing labels\n",
        "\n",
        "print(\"Accuracy: \", KNN.score(x_test, y_test)) # Print the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### MAIN #####\n",
        "# Opening cv2 Webcam\n",
        "\n",
        "with open(\"aslearn.ai\", \"rb\") as f:  # Open the file to read\n",
        "    KNN = pickle.load(f)  # Read the classifier from the file\n",
        "\n",
        "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW) # Open the webcam #! CHANGE THE CAMERA INDEX IF DOESN'T WORK\n",
        "detector = HandDetector(detectionCon=0.8, maxHands=1)  # Create a hand detector\n",
        "\n",
        "try:\n",
        "    while cv2.waitKey(1) != ord('q'):  # Loop until the user presses 'q'\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        ret, frame = cap.read()  # Read camera frame\n",
        "\n",
        "        hands = detector.findHands(frame, draw=False)  # Find a hand\n",
        "        # if it is left hand\n",
        "        if (hands and hands[0] and hands[0]['type'] == 'Left'):\n",
        "            mirror_frame = cv2.flip(frame, 1)\n",
        "            hands = detector.findHands(mirror_frame, draw=False)  # Find a hand\n",
        "\n",
        "        if not (hands and hands[0]):  # if no hands found\n",
        "            print('nothing: 100%', flush=True) # Print nothing if no hand is detected\n",
        "            continue  # go to the next frame\n",
        "\n",
        "        x, y, w, h = hands[0]['bbox']  # Get the bounding box\n",
        "        # cx, cy = hands[0]['center']\n",
        "\n",
        "        x -= 20  # Move the bounding box to the center of the hand\n",
        "        y -= 20  # Move the bounding box to the center of the hand\n",
        "        w += 40  # Expand the bounding box\n",
        "        h += 40  # Expand the bounding box\n",
        "\n",
        "        if (w > h):  # if the width is greater than the height\n",
        "            h = w  # Make the height the same as the width\n",
        "        if (h > w):  # if the height is greater than the width\n",
        "            w = h  # Make the width the same as the height\n",
        "\n",
        "        hand_img = frame[y:y+h, x:x+w]  # Crop the hand out of the frame\n",
        "        try:\n",
        "            cv2.imshow(\"HAND\", cv2.flip(hand_img, 1))  # Show the image\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        cv2.setWindowProperty(\"HAND\", cv2.WND_PROP_TOPMOST, 1)  # Set the window to the topmost\n",
        "\n",
        "        insertData = hands[0]['lmList']  # Get the landmarks\n",
        "\n",
        "        for ins_data in insertData:  # move each landmark according to the bbox of the hand\n",
        "            ins_data[0] -= x\n",
        "            ins_data[1] -= y\n",
        "\n",
        "        insertData = np.asarray(insertData).reshape(1, -1)  # reshape the landmarks to a 1D numpy array\n",
        "\n",
        "        x1, y1, w1, h1 = hands[0]['bbox']  # Get the bounding box\n",
        "        x1 -= x  # Move the bounding box according to the bbox of the hand\n",
        "        y1 -= y  # Move the bounding box according to the bbox of the hand\n",
        "        insertData = np.append(insertData, (x1, y1, w1, h1))  # Append the bounding box to the landmarks\n",
        "        insertData = np.asarray(insertData).reshape(1, -1)  # reshape the bounding box to a 1D numpy array\n",
        "\n",
        "        cx1, cy1 = hands[0]['center']  # Get the center\n",
        "        cx1 -= x  # Move the center according to the bbox of the hand\n",
        "        cy1 -= y  # Move the center according to the bbox of the hand\n",
        "        insertData = np.append(insertData, (cx1, cy1))  # Append the center to the landmarks\n",
        "        insertData = np.asarray(insertData).reshape(1, -1)  # reshape the center to a 1D numpy array\n",
        "\n",
        "        insertData = np.append(insertData, hands[0]['type'] == 'Right')  # insert 1 if the hand is right, 0 if the hand is left\n",
        "\n",
        "        predict = KNN.predict_proba([insertData])  # Predict the probability of each label\n",
        "\n",
        "        predict = [int(i*100) for i in predict[0]]  # Convert the probabilities to integers\n",
        "        precentageDict = {}\n",
        "        for i, percentage in enumerate(predict):  # For each percentage\n",
        "            if (percentage > 0.):\n",
        "                precentageDict[labels[i]] = int(percentage)  # Add the label and the percentage to the dictionary\n",
        "        precentageDict = {k: v for k, v in sorted(precentageDict.items(), key=lambda item: item[1], reverse=True)}  # Sort the dictionary by the percentage\n",
        "\n",
        "        print('\\n'.join([key + ': ' + str(value) + '%' for key, value in precentageDict.items()]), flush=True)  # Print the dictionary\n",
        "\n",
        "finally:\n",
        "    cap.release()  # Release the webcam\n",
        "    cv2.destroyAllWindows()  # Close all the windows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflowjs as tfjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(63,)\n"
          ]
        }
      ],
      "source": [
        "# KNN = KNeighborsClassifier(n_neighbors=5) # Create a KNN classifier\n",
        "x_train = traindots_df.copy()\n",
        "# x_train.drop(['63', '64', '65', '66', '67', '68', '69'], axis=1)\n",
        "x_train = tf.cast(x_train.iloc[:, 1:64].values, 'int32') # Get the training data\n",
        "print(x_train[0].shape)\n",
        "# x_train = x_train.loc[:, traindots_df.columns != 'label'].values # Get the training data\n",
        "y_train = tf.cast(traindots_df['label'].values, 'int32') # Get the training labels\n",
        "\n",
        "# KNN.fit(x_train, y_train) # Train the classifier\n",
        "\n",
        "# with open(\"aslearn.ai\", \"wb\") as f: # Open the file to write\n",
        "    # pickle.dump(KNN, f) # Write the classifier to the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2419/2419 [==============================] - 3s 1ms/step - loss: 1.6451 - accuracy: 0.8132\n",
            "Epoch 2/10\n",
            "2419/2419 [==============================] - 3s 1ms/step - loss: 0.2947 - accuracy: 0.9335\n",
            "Epoch 3/10\n",
            "2419/2419 [==============================] - 2s 949us/step - loss: 0.1579 - accuracy: 0.9579\n",
            "Epoch 4/10\n",
            "2419/2419 [==============================] - 2s 958us/step - loss: 0.1112 - accuracy: 0.9668\n",
            "Epoch 5/10\n",
            "2419/2419 [==============================] - 2s 932us/step - loss: 0.0986 - accuracy: 0.9696\n",
            "Epoch 6/10\n",
            "2419/2419 [==============================] - 2s 968us/step - loss: 0.0903 - accuracy: 0.9724\n",
            "Epoch 7/10\n",
            "2419/2419 [==============================] - 2s 954us/step - loss: 0.0845 - accuracy: 0.9747\n",
            "Epoch 8/10\n",
            "2419/2419 [==============================] - 3s 1ms/step - loss: 0.0746 - accuracy: 0.9766\n",
            "Epoch 9/10\n",
            "2419/2419 [==============================] - 2s 965us/step - loss: 0.0756 - accuracy: 0.9776\n",
            "Epoch 10/10\n",
            "2419/2419 [==============================] - 2s 953us/step - loss: 0.0673 - accuracy: 0.9798\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1fbbd808d00>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model = tf.keras.Sequential([\n",
        "#   layers.Dense(128, activation='relu'),\n",
        "#   layers.Dense(29)\n",
        "# ])\n",
        "\n",
        "# model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(29)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.save('server/tfjs')\n",
        "\n",
        "tf.keras.models.save_model(\n",
        "    model,\n",
        "    'server/tfjs.h5',\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format='h5',\n",
        "    signatures=None,\n",
        "    options=None,\n",
        "    save_traces=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfjs.converters.save_keras_model(model, 'server/tfjs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.load_model('server/tfjs.h5')\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Z\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "print(labels[25])\n",
        "print(labels.index('F'))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "15b78ae7d8bb541448fc574e994727730f3ee429e4f78406e0bebe36fe529ad6"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
